# **Index**

| −2 ln L, see deviance                           | Bernoulli process, 141                                |
|-------------------------------------------------|-------------------------------------------------------|
| χ2,<br>286                                      | best-fitting parameter estimates, 52                  |
| -greedy choice, 400, 403                        | Beta function, 135                                    |
|                                                 | bias unit, in backpropagation, 359                    |
| ABC, 163–170, 233                               | bias-variance trade-off, 245–247, 392                 |
| overview, 164                                   | BIC, 117, 271, 297–301, 315, 319                      |
| recognition memory example, 166                 | Bonini's paradox, 323, 324                            |
| sampling, 164                                   | bootstrapping, 65–70, 119                             |
| summary statistic, 165                          | boundary separation, 29, 122, 371, 387, 420           |
| accumulation rate, see drift rate               | Brain-State-in-a-Box model, see BSB                   |
| accumulator, 37, 123, 387, 389, 410, 411, 419   | BSB, 352–356                                          |
| activation, 14–17, 20, 21, 41–43, 90, 334       | buckets, 92                                           |
| activation function                             | burnin, 151, 175, 178, 180, 209                       |
| linear, 337                                     | burst vs buildup cells, 415                           |
| logistic, 357                                   |                                                       |
| AIC, 258–261, 271, 273, 297, 299–302, 315, 319  | catastrophic interference, in backpropagation         |
| differences, 259                                | learning, 365                                         |
| AIC vs BIC, 299–301                             | categorization, 7–9, 13–17, 75, 353                   |
| AIC with small sample correction, see AICc      | distance and angle, 14                                |
| AICc, 261                                       | of faces, 7, 13, 14, 16–17, 87                        |
| Akaike weights, 260–261                         | of line lengths, 14                                   |
| Akaike's Information Criterion, see AIC         | χ2 distribution,<br>see probability distribution      |
| amnesia, 317, 318                               | χ2,<br>249, 255                                       |
| Approximate Bayesian Computation, see ABC       | choice RT, 24–26, 85, 86, 369–392                     |
| artificial intelligence, 324                    | choice task, 24–26, 369–392                           |
| associative memory, 334                         | classification, see categorization                    |
| attentional refreshing, 41                      | cognitive load, 41                                    |
| auto-associator, 314, 349–356                   | cognitive psychometrics, 121, 122                     |
| autocorrelations, 147                           | coin tossing, 90                                      |
| auxiliary assumptions, 311, 319, 333            | common coding, 324                                    |
| averaged data, 106–109                          | conceptual innovation, 313                            |
| averaged distributions, see Vincentized average | conceptual simulation, 313                            |
|                                                 | conditional distributions, 172, 175, 176              |
| backpropagation network, 356–365                | conditional probability, see probability, conditional |
| in psychology, 364–365                          | confidence interval, 67                               |
| internal representations, 363                   | conjugacy, 136                                        |
| backpropagation of error, 361                   | continuous variable, 76–78, 82, 204                   |
| backpropagation through time, 364               | convergence problem, 161                              |
| bandit task, 398                                | Copernicus, 3–6                                       |
| Bayes factor, 276–277                           | CRAN, 26                                              |
| Bayes' theorem, 127, 128, 135, 273              | cross-entropy, 256                                    |
| Bayesian Information Criterion, see BIC         | cross-modal integration, 244                          |
| Bayesian parameter estimation, 102, 126–139     | cross-validation, 262, 302, 321, 418                  |
|                                                 |                                                       |

| cued recall, 334, 336, 342                           | Expectation-Maximization (EM), 115–117              |
|------------------------------------------------------|-----------------------------------------------------|
| cumulative distribution function, 77, 79, 98, 168,   | exponential function                                |
| 183, 220, 376                                        | relationship to logarithm, 95, 249                  |
| cumulative prospect theory, see prospect theory      | exponential learning, 10–11                         |
|                                                      | express saccades, 114                               |
| Darwin's theory of evolution, see evolution          | EZ diffusion model, 392                             |
| data averaging, see averaged data                    |                                                     |
| data description, 12                                 | falsifiability, 18–20, 264, 317, 385                |
| data model, 91                                       | FEF, 410, 411, 413                                  |
| data vector, 82                                      | Fisher information, 263, 272, 295, 297, 299         |
| debugging, 327–328                                   | fMRI, 409, 411–415, 418                             |
| deep belief networks, 365                            | forgetting, 12, 211, 320                            |
| delay discounting, 218, 219, 222, 418                | exponential function, 212, 278, 287, 292            |
| derivative, 79, 263, 268, 295, 296, 361              | interference-based, 320                             |
| determinant, 272, 296, 297                           | power function, 212, 278, 287, 292                  |
| deviance, 96, 249, 256, 258, 297, 298                | time-based, 320                                     |
| DIC, Deviance Information Criterion, 302, 419        | free recall, 118, 316                               |
| differential evolution, ABCDE, 170                   | frontal eye field, see FEF                          |
| diffusion model, 121, 316, 369, 412, 414, 419        | functional magnetic resonance imaging,              |
| Bayesian approaches, 392                             | see fMRI                                            |
| drift rate, 370                                      |                                                     |
| EZ, 392                                              | G2,<br>249, 257                                     |
| falsifiability, 385                                  | Gaussian mixture model, 113–117                     |
| fitting, 371                                         | Gaussian quadrature, 278                            |
| interpretation, 383                                  | GCM, 8, 13–18, 75–76, 87–91, 97–101, 271, 307,      |
| quantile probability functions, 371                  | 316, 320, 397                                       |
|                                                      |                                                     |
| diminishing utility, 250                             | generalization, 345                                 |
| discount factor, 402                                 | generalized linear mixed models, 228                |
| discounting of the future, future discounting,       | Gibbs sampling, 172–177                             |
| see delay discounting                                | ABC, 234                                            |
| discrepancy function, 47–49, 62                      | bivariate example, 173                              |
| discrete variable, 75, 84, 204, 256                  | vs. Metropolis-Hastings, 176                        |
| distance, 14–15                                      | git, 328–330                                        |
| distributed representation, 335                      | collaboration, 329                                  |
| distribution                                         | global minimum, 60                                  |
| posterior, 137                                       | goodness-of-fit, 258, 260                           |
| dopamine neurons, 404, 405, 409                      | gradient descent, 361, 365                          |
| drift diffusion model, 121, 122                      | graphical models, 204–206                           |
| drift rate, 26, 28, 31, 32, 34, 236, 316, 370, 376,  | signal-detection, 204                               |
| 384–387, 411–415, 420                                | grid search, 52, 59, 70                             |
|                                                      | grouping, in memory, 119                            |
| EEG, 414, 418–420                                    |                                                     |
| eigenvalue, 351                                      | harmonic mean estimator, 281                        |
| eigenvector, 351, 353                                | Hebbian associator                                  |
| eligibility traces, 404, 409                         | limitations, 356                                    |
| EM, see Expectation-Maximization                     | Hebbian learning, 337, 338, 349                     |
| error surface, 51, 59                                | limitations, 356                                    |
| errors                                               | heliocentric model, 3, 5, 6                         |
| fast, 33, 35                                         | Hessian matrix, 271, 272, 295, 296                  |
| slow, 33, 36                                         | hierarchical model, 22, 23, 103, 122, 203–234, 287, |
| Euclidian distance, 89, 90, 339                      | 301–303                                             |
| event, 73                                            | hierarchical modeling                               |
| evidence, 19, 24–26, 28, 37, 72, 128, 134, 147, 172, | advantages, 233                                     |
| 256, 273, 274, 276, 300, 319, 386, 387               | concepts, 203                                       |
| evolution, 18                                        | forgetting, 211                                     |
| exemplar model, 8, 9, 13, 18, 75, 320, 397           | inter-temporal preferences, 218                     |

| maximum likelihood, 228                             | likelihood surface, 95                        |
|-----------------------------------------------------|-----------------------------------------------|
| shrinkage, 211                                      | multiple parameters, 94                       |
| signal detection, 207                               | multiple participants, 93, 94                 |
| Bayesian vs. maximum likelihood, 233                | likelihood function, 82, 95, 128, 130, 131    |
| maximum likelihood, 228                             | regularity, 101                               |
| Stein's paradox, 211                                | likelihood ratio, 249, 260                    |
| high-threshold model, 186, 269, 281, 284            | likelihood ratio test, 249, 255               |
| hyper-hyperparameters, 203                          | Likert scale, 48, 75                          |
| hyperparameters, 152, 168, 417, 418                 | Linear Ballistic Accumulator, see LBA         |
|                                                     | linear ballistic accumulator, see LBA         |
| identifiability, 102, 264–269, 315                  | linear regression, 50                         |
| identifiability problem, 321                        | LIP, 411, 419                                 |
| importance sampling, 280–284                        | local minimum, 60                             |
| indicator variable, 287, 294                        |                                               |
| individual differences, 94, 121–122, 206, 209, 211, | local necessity, 319–321                      |
| 226, 233, 234, 316, 385                             | localist representation, 335                  |
| information, 144, 256, 297–299                      | log-likelihood, 95–258                        |
| integration, 79, 263, 273–303                       | additive properties, 95                       |
| inter-temporal preference, 218, 226, 418            | joint, 96, 252                                |
| IPS, 413                                            | negative, 97                                  |
| irrelevant specification problem, 320               | simplifying, 96–261                           |
|                                                     | log-likelihood function                       |
| Jacobian matrix, 268, 269, 315                      | normal, 96                                    |
| JAGS, 163, 172                                      | logarithm, 95                                 |
| convergence diagnostics, 184                        | logarithm function, 260                       |
| declarative language, 179                           | natural log, 95                               |
| forgetting, 212                                     | relationship to exponential, 95               |
| installation, 177                                   | loss aversion, 251, 255                       |
| inter-temporal preferences, 221                     | LRT, see likelihood ratio test                |
| signal-dectection model, signal-dectection model,   | Luce's choice rule, 16, 17, 75                |
| 182                                                 |                                               |
| specify prior distribution, 180                     | Makefile, 331                                 |
| joint probability, see probability, joint           | marginal likelihood, 130, 134–135, 147, 172,  |
|                                                     | 273–274, 276, 277, 277–303                    |
|                                                     | marginal probability, 129                     |
| K-means clustering, 118–120                         | Markov Chain, 147                             |
| gap statistic, 119                                  | Marr's levels of analysis, 396, 422           |
| Kepler's model, 6                                   | matrix algebra, 339–342, 348–349              |
| knitr, 330                                          | addition and subtraction, 339                 |
| Kullback-Leibler distance, 256–258                  | distributive property, 348                    |
| expected, 258, 259, 261                             | inner product, 341, 345                       |
|                                                     | multiplication, 340, 344                      |
| Laplace approximation to marginal likelihood,       | non-commutative, 340                          |
| 294–297                                             | outer product, 341                            |
| latency, 102, 103, 121                              | maximum likelihood estimation, 95–103, 126    |
| LATER model, Linear Approach to Threshold with      |                                               |
| Ergodic Rate, 410                                   | asymptotic normality, 102                     |
| lateral interparietal cortex, see LIP               | biased estimators, 102                        |
| LBA, 316, 386–392, 412–414, 417–419                 | bounded parameters, 99                        |
| fitting, 388                                        | consistency of estimators, 102                |
| learning rate, 337, 357, 400                        | efficiency of estimators, 102                 |
| likelihood, 80–128                                  | fitting individuals, 111–112                  |
| vs. probability, 83                                 | minimization vs. maximization, 95             |
| colloquial usage, 72                                | normalized, see normalized maximum likelihood |
| definition, 82                                      | overdispersion of estimates, 103              |
| fitting, see maximum likelihood estimation          | parameterization invariance, 102              |
| joint, 93                                           | properties of estimators, 101                 |
|                                                     |                                               |

| MCMC, 146                                               | momentum, in learning by backpropagation,                  |
|---------------------------------------------------------|------------------------------------------------------------|
| autocorrelations, 147, 162                              | 361                                                        |
| chain, 149, 158, 175                                    | Monte Carlo integration, 280                               |
| chain, MCMC                                             | Monte Carlo methods, 146                                   |
| convergence, 183                                        | MPT, 22, 186                                               |
| convergence diagnostics, 162, 184                       | eyewitness testimony, 190                                  |
| convergence problem, 161                                | no-conflict model, JAGS                                    |
| fit mixture model, 156                                  | no-conflict model, 192                                     |
| JAGS, 163, 172                                          | multidimensional scaling, 88, 363                          |
| Metropolis-Hastings, 148                                | multilevel modeling, 203                                   |
| multiple chains, MCMC                                   | advantages, 233                                            |
| starting values, 161                                    | concepts, 203                                              |
| problems, 161                                           | forgetting, 211                                            |
| proposal distribution, 148                              | inter-temporal preferences, 218                            |
| scale reduction factor, 186                             | maximum likelihood, 228                                    |
| shrink factor, 186                                      | parent distribution, 203                                   |
| target distribution, 149                                | shrinkage, 211                                             |
| thinning, 162                                           | signal detection, 207                                      |
| tuning parameter, 149                                   | Bayesian vs. maximum likelihood, 233                       |
| measurement model, 265, 270, 315                        | maximum likelihood, 228                                    |
| mental rotation, 419                                    | Stein's paradox, 211                                       |
| mercurial, 328                                          | multinomial distribution, see probability                  |
| Metropolis-Hastings algorithm, 148                      | distribution, multinomial                                  |
| minimum description length, 262–263,                    | multinomial processing tree, see MPT                       |
| 272, 296                                                | multiple participants                                      |
| mixture model, 113–117, 154–160, 180, 235               | likelihood, see likelihood, multiple participants          |
| Gaussian, see Gaussian mixture model                    | mutual exclusivity, 73                                     |
| model, 6                                                |                                                            |
| choice task, 24                                         | NaN, not a number, 328                                     |
| cognitive aid, 20–22                                    | necessity, 318–321                                         |
| descriptive, 9–13                                       | local, see local necessity                                 |
| multinomial processing tree, 22                         | nested models, 249, 255, 284, 300, 320                     |
| process explanation, 17                                 | neural network, 314, 316                                   |
| quantitative, 5                                         | activation, 335, 336                                       |
| random-walk, 25–37                                      | graceful degradation, 346                                  |
| boundary, 28                                            | input layer, 335                                           |
| drift rate, 28, 32                                      | lesioning, 345                                             |
| Trial-to-trial variability, 33                          | output layer, 335                                          |
| sequential-sampling, 33, 37, 369                        | pattern of activation, 335                                 |
| toolkit                                                 | neuroscience, criticisms of, 395                           |
| discrepancy function, 47–49                             | normalized maximum likelihood, 263                         |
| vs. intuition, 21                                       | normative behavior, normative expectations, 12             |
| model comparison, 6, 9, 103, 248, 276,<br>319, 320, 332 | null hypothesis, 186, 249, 255, 256, 284, 285,<br>301, 304 |
| model complexity, 243–245, 247, 258, 274–276,           | numerical integration, 278–279                             |
| 300, 315                                                | numerical overflow, 299                                    |
| out of set prediction, 247                              |                                                            |
| polynomial, 243–244                                     | Occam's razor, 244, 274                                    |
| model exploration, 312–314, 324                         | outcome, 73, 80                                            |
| model flexibility, see model complexity,                | outcome space, 19                                          |
| see flexibility                                         | output interference, 320                                   |
| model weights, 261                                      | over-fitting, 243, 245                                     |
| model-based cognitive neuroscience, 396                 |                                                            |
| modeling                                                | parameter estimates                                        |
| toolkit, 38                                             |                                                            |
| parameters, 38                                          | best-fitting estimates, 52<br>variability, 65              |
|                                                         |                                                            |

**Index** 459

| parameter estimation, 50–64, 257             | pseudoprior, see pseudoprior                        |
|----------------------------------------------|-----------------------------------------------------|
| bounded parameters, 99                       | reference, 142                                      |
| by eye, 321                                  | sensitivity analysis, see sensitivity analysis      |
| error surface, 51                            | specification of, 299, 303–305                      |
| general limitation, 60                       | subjective, 303, 304                                |
| global minimum, 60                           | unit information, 297–299                           |
| grid search, 52                              | prior distribution, 102, 131                        |
| histogram of estimates, 67                   | uniform, 140                                        |
| least-squares, 50                            | prior predictive, 304                               |
| local minimum, 60                            | prior predictive distribution, 304                  |
| maximum likelihood, 50, see maximum          | prior probability, 128                              |
| likelihood estimation, 126                   | priority heuristic, 258–259, 298                    |
| parameter estimates                          | probability, 72, 80, 92                             |
| variability, 65                              | axioms, 73                                          |
| simulated annealing, 61                      | conditional, 74, 126                                |
| starting values, 56                          | definition, 72                                      |
| parameter recovery, 315                      | joint, 73, 74, 82, 127, 130                         |
| parameter vector, 80                         | marginal, 129                                       |
| parameters, 10, 38, 39                       | of observed data, 128                               |
| best-fitting, 39                             | posterior, 129                                      |
| continuous, 59                               | prior, 128                                          |
| contribution to model behavior, 314–315      | properties of, 73–75                                |
| discrete, 59                                 | probability density function, 78–80, 85, 95, 256    |
| fixed, 39                                    | probability distribution, 75–80                     |
| free, 39                                     | χ2,<br>249                                          |
| parametric resampling, 65                    | Bernoulli, 94, 130, 131, 222                        |
| parent distribution, 203, 207                | beta, 132–135, 141, 144, 146, 284, 288, 292, 301    |
| parsimony, 39, 258, 274, 276, 300, 322, 323  | bimodal, 113                                        |
| phasic bursts, in dopamine neurons, 405, 408 | binomial, 67, 75, 84, 85, 90, 91, 93, 95, 131, 205, |
| plain text, vs rich text, 326                | 211, 214, 286, 288                                  |
| planetary motion, retrograde motion, 3       | ex-Gaussian, 102                                    |
| posterior distribution, 135, 137             | exponential, 95                                     |
| posterior predictive distribution, 215       | multinomial, 92–93, 95, 261                         |
| posterior probability, 129                   | multivariate Gaussian, 296                          |
| power law, 10–12, 108                        | multivariate normal, 173, 298                       |
| pre-SMA, 412, 414, 417, 420                  | normal, 35, 95, 96, 98, 102, 115, 151, 166, 168,    |
| pre-supplementary motor area, see pre-SMA    | 189, 200, 201, 220, 242, 244, 274, 302,             |
| precision (vs. standard deviation), 159      | 387, 398                                            |
| prediction error, 397, 400, 405, 408, 409    | von Mises, 157                                      |
| present subjective value, 219                | Wald, 77, 78, 86, 94                                |
| primacy gradient, 42                         | Weibull, 109                                        |
| prior, 411                                   | probability distribution function, see cumulative   |
| conjugate, 136                               | distribution function                               |
| data-informed, 299, 304                      | probability functions, 75–80                        |
| determining, 139                             | probability integral transform, 169                 |
| Haldane, 142, 303                            | probability mass function, 75–76, 80, 85, 92, 95    |
| hyperparameters, 152                         | probit regression, 230                              |
| hyperparameters, ABC                         | process models, 13                                  |
| hyperparameters, 168                         | product space method, 287                           |
| improper, 303                                | proposal distribution, 148, 149                     |
| informative, 304, 305                        | prospect theory, 249–255, 298                       |
| Jeffrey's prior, 263                         | cumulative, 251                                     |
| Jeffreys, prior                              | probability weighting function, 251                 |
| non-informative, 159                         | value function, 250                                 |
| non-informative, 303, 305                    | prototype model, 9, 271                             |
| non-uniform, 151                             | pseudoprior, 291–292                                |

| psychophysics, 241                             | sample space, 73                                    |
|------------------------------------------------|-----------------------------------------------------|
| polynomial law, 241–243                        | sampling distribution, 65, 67, 255                  |
| Ptolemaic system, 3, 5, 6                      | sampling variability, 79, 90                        |
| Pythagorean theorem, 15                        | SARSA, 403–404                                      |
|                                                | SARSA(λ), 403                                       |
| Q values, 399–401                              | Savage-Dickey ratio, 284, 301                       |
| quantile, 67–68                                | scientific reasoning, 20                            |
| quantile probability functions, 371            | scope, 18–20                                        |
| quantile probability plots, 372                | sensivitiy analysis, 304                            |
| quasi-identifiability, 266                     | sequential-sampling models, 33, 37, 369             |
|                                                | serial recall, 314                                  |
| R, 26                                          | sharing code, 325–326, 329                          |
| apply, 89, 111                                 | short-term memory, 320, 321                         |
| drawing random samples, 28                     | shrinkage, 211                                      |
| lapply, 90                                     | signal detection theory, 166, 269, 281, 285, 302,   |
| log, 95                                        | 315, 317                                            |
| loops, 28                                      | similarity, 14–15                                   |
| matrix, 29                                     | SIMPLE, 320                                         |
| packages, 331                                  | Simplex, 57–60, 96                                  |
| packrat, 332                                   | limitations, 59                                     |
| pnorm, 98, 286                                 | Simpson's paradox, 106                              |
| sample, 400                                    | simulated annealing, 61                             |
| sourcing, 87                                   | acceptance function, 61, 62                         |
| random walk, 25–37                             | candidate function, 61                              |
| re-paramaterization, 266–267                   | cooling schedule, 63                                |
| reading, 316–317                               | temperature, 62                                     |
| reasoning, 121                                 | single cell recording, 410, 411, 415–417            |
| recency effect, 314                            | skill acquisition, 10–12                            |
| recognition, 7–9, 166, 264, 317, 318           | slow errors, 36                                     |
| familiarity, 166, 318                          | softmax, 252                                        |
| reduced model, see nested models               | speed-accuracy trade-off, 412, 417                  |
| reference priors, 142                          | speeded-choice task, 24–26, 369–392                 |
| regression, 50                                 | spike train, see single cell recording              |
| regularity effect, 317, 357, 362               | spreading activation theory, 20, 21                 |
| reinforcement learning, 398–410                | stage-like behavior, 317                            |
| action value learning, 398–401                 | Stan, 172                                           |
| future expected reward, 402                    | starting point, 31, 34, 35, 370, 371, 375, 386–388, |
| state-action value learning, 401–404           | 392, 411–413                                        |
| rejection sampling, 170                        | starting values, 52, 56, 100, 161                   |
| relative strength of evidence, see strength of | Stein's paradox, 211                                |
| evidence                                       | Sternberg task, 264                                 |
| remember-know, 317                             | strength of evidence, 256, 260, 261, 319            |
| replicability, see reproducibility             | striatum, 409, 412, 417, 420                        |
| repository, 26, 329, 330                       | strong inference, 318                               |
| reproducibility, 20                            | structural equation modeling (SEM), 121, 316        |
| resampling procedures, see bootstrapping       | Subplex, 60                                         |
|                                                |                                                     |
| response boundary, 28                          | sufficiency, 316–318                                |
| response suppression, 314–315                  | superior colliculus, 411                            |
| response time, see latency                     |                                                     |
| RMSD, 48, 50, 169                              | taxation, 10                                        |
| RNG, random number generator, 331              | Taylor series expansion, 295                        |
| rolling dice, 18                               | TBRS, 41                                            |
| RStudio, 26                                    | TBRS*, 41                                           |
|                                                | temporal difference learning, 403                   |
| saccadic decision-making, 410–411              | testability, see falsifiability, 269–270, 315       |
| sample, 73                                     | thinning, 162                                       |

thought experiment, *see* conceptual simulation threshold, 25, 26, 42, 77, 86, 97, 123, 187, 236, 318, 359, 370, 387, 388, 410–415 time-based resource-sharing model, *see* TBRS toolkit, 38 discrepancy function, 47–49 parameters, 38 tract strength, 417 Transdimensional MCMC, 287–294 Trial-to-trial variability, 33, 370, 371, 392 drift rate, 36, 37, 370 starting point, 34, 370 truth, 256, 257, 322 tuning parameter, 149

uncertainty, 248, 256 unlearning, 314 utility, 252

value function, 250 vector cosine, 338 vector length, *see* vector norm vector norm, 339, 341 vector normalization, 339 verbal theorizing, 6, 9, 41, 311 verisimilitude, 323 version control, 328–329 Vincentized average, 109–111 visual working memory, 154

WAIC, widely applicable information criterion, 302 Walsh matrix, 349 Widrow-Hoff learning, 360 WinBUGS, 172 word frequency, 317 word naming, 316, 317, 356 working memory, 121, 154